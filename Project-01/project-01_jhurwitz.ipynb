{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1: Word counts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part A. Characters in Little Women*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How many times are each of the following characters mentioned by name in the text of Little Women?:* \n",
    "\n",
    "*Jo, Beth, Meg, Amy*\n",
    "\n",
    "*Use the text available at https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-andschedule/\n",
    "master/projects/project-01/women.txt for this part.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Retrieve text of Little Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-23 01:59:06--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1053440 (1.0M) [text/plain]\n",
      "Saving to: ‘women.txt.1’\n",
      "\n",
      "women.txt.1         100%[=====================>]   1.00M  --.-KB/s   in 0.05s  \n",
      "\n",
      "2016-09-23 01:59:06 (22.0 MB/s) - ‘women.txt.1’ saved [1053440/1053440]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Produce counts of the names 'Jo', 'Beth', 'Meg', 'Amy'. Consider matches regardless of upper/lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1362 jo\r\n",
      "    686 meg\r\n",
      "    652 amy\r\n",
      "    467 beth\r\n"
     ]
    }
   ],
   "source": [
    "!grep -oE '\\w{{2,}}' women.txt |\\\n",
    "    tr '[:upper:]' '[:lower:]' |\\\n",
    "    grep -w 'jo\\|beth\\|meg\\|amy' |\\\n",
    "    sort |\\\n",
    "    uniq -c |\\\n",
    "    sort -rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'grep -w' command is used to limit results to only the exact matches of each name. Without using this, the grep was returning hits for any words that contained the target strings. After applying this function, I sorted by name and retrieved the unique counts. Finally, I presented these counts in descending order, which I have reproduced below:\n",
    "\n",
    "Jo: 1,362  \n",
    "Meg: 686  \n",
    "Amy: 652  \n",
    "Beth: 467"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part B. Juliet and Romeo in Romeo and Juliet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How many times do each of the characters Juliet and Romeo have speaking lines in Romeo and Juliet? Keep\n",
    "in mind that this is the text of a play.\n",
    "Use the text available at https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-andschedule/\n",
    "master/projects/project-01/romeo.txt for this part.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Retrieve text of Romeo and Juliet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-16 02:26:31--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 178983 (175K) [text/plain]\n",
      "Saving to: ‘romeo.txt’\n",
      "\n",
      "romeo.txt           100%[=====================>] 174.79K  --.-KB/s   in 0.02s  \n",
      "\n",
      "2016-09-16 02:26:32 (7.50 MB/s) - ‘romeo.txt’ saved [178983/178983]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Calculate number of times Juliet and Romeo have speaking lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    117 Jul\r\n",
      "    163 Rom\r\n"
     ]
    }
   ],
   "source": [
    "!grep -oE '\\w{{2,}}' romeo.txt |\\\n",
    "    grep -w 'Rom\\|Jul' |\\\n",
    "    sort |\\\n",
    "    uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of speaking lines, I used 'Rom' and 'Jul' as the search terms. I used these because in reviewing the text, each time either of these characters had a speaking line, it was prefixed by either 'Rom.' or 'Jul.'. Again, I used the '-w' command to limit it to exact matches. The exact number of times each character spoke is reproduced below:\n",
    "\n",
    "Juliet: 117  \n",
    "Romeo: 163"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Problem 2: Capital Bikeshare **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part A: Which 10 Capital Bikeshare stations were the most popular departing stations in Q1 2016? Which 10 were the most popular destination stations in Q1 2016?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-18 02:19:12--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10643003 (10M) [application/octet-stream]\n",
      "Saving to: ‘2016q1.csv.zip’\n",
      "\n",
      "2016q1.csv.zip      100%[=====================>]  10.15M  16.3MB/s   in 0.6s   \n",
      "\n",
      "2016-09-18 02:19:13 (16.3 MB/s) - ‘2016q1.csv.zip’ saved [10643003/10643003]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Unzip file downloaded in Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  2016q1.csv.zip\n",
      "  inflating: 2016q1.csv              \n"
     ]
    }
   ],
   "source": [
    "!unzip 2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: View columns in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Duration (ms)\r\n",
      "  2: Start date\r\n",
      "  3: End date\r\n",
      "  4: Start station number\r\n",
      "  5: Start station\r\n",
      "  6: End station number\r\n",
      "  7: End station\r\n",
      "  8: Bike number\r\n",
      "  9: Member Type\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n 2016q1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: View sample data for fields: Start date, End date, Start Station, End station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|------------------+-----------------+--------------------------------+-------------------------------+--------------|\r\n",
      "|  Start date      | End date        | Start station                  | End station                   | Bike number  |\r\n",
      "|------------------+-----------------+--------------------------------+-------------------------------+--------------|\r\n",
      "|  3/31/2016 23:59 | 4/1/2016 0:04   | 11th & S St NW                 | 1st & Rhode Island Ave NW     | W00022       |\r\n",
      "|  3/31/2016 23:59 | 4/1/2016 0:08   | New Hampshire Ave & 24th St NW | 18th St & Wyoming Ave NW      | W01294       |\r\n",
      "|  3/31/2016 23:59 | 4/1/2016 0:08   | 14th & V St NW                 | 18th & M St NW                | W01416       |\r\n",
      "|  3/31/2016 23:57 | 4/1/2016 0:09   | 34th St & Wisconsin Ave NW     | 17th & Corcoran St NW         | W01090       |\r\n",
      "|  3/31/2016 23:57 | 3/31/2016 23:59 | 23rd & Crystal Dr              | 27th & Crystal Dr             | W21934       |\r\n",
      "|  3/31/2016 23:57 | 4/1/2016 0:13   | 11th & M St NW                 | 5th & K St NW                 | W20562       |\r\n",
      "|  3/31/2016 23:57 | 4/1/2016 0:06   | New York Ave & 15th St NW      | 18th & R St NW                | W20222       |\r\n",
      "|  3/31/2016 23:56 | 4/1/2016 0:00   | 8th & H St NW                  | 5th & K St NW                 | W20291       |\r\n",
      "|  3/31/2016 23:55 | 4/1/2016 0:01   | Columbia Rd & Belmont St NW    | 20th & O St NW / Dupont South | W20590       |\r\n",
      "|------------------+-----------------+--------------------------------+-------------------------------+--------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c2,3,5,7,8 2016q1.csv | head -10 | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Confirm that all End dates occurred in Q1 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. End\n",
      "\t<class 'datetime.date'>\n",
      "\tNulls: False\n",
      "\tMin: 2016-01-01\n",
      "\tMax: 2016-04-01\n",
      "\tUnique values: 88\n",
      "\t5 most frequent values:\n",
      "\t\t2016-03-26:\t14102\n",
      "\t\t2016-03-16:\t13355\n",
      "\t\t2016-03-24:\t13341\n",
      "\t\t2016-03-23:\t12332\n",
      "\t\t2016-03-25:\t12224\n",
      "  2. date\n",
      "\t<class 'datetime.time'>\n",
      "\tNulls: False\n",
      "\tMin: 00:00:00\n",
      "\tMax: 23:59:00\n",
      "\tUnique values: 1440\n",
      "\t5 most frequent values:\n",
      "\t\t17:53:00:\t1193\n",
      "\t\t17:54:00:\t1187\n",
      "\t\t17:44:00:\t1182\n",
      "\t\t17:43:00:\t1177\n",
      "\t\t17:56:00:\t1176\n",
      "\n",
      "Row count: 552399\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c3 2016q1.csv | csvstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Save sorted list of [start stations] as csv file. Due to the file size and processing time, I've included this intermediate step of saving the preliminary output to a csv file -- which I repeat for similar exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!csvcut -c5 2016q1.csv |\\\n",
    "    sort > output1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Return top 10 most popular departing stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13120 Columbus Circle / Union Station\r\n",
      "   9560 Massachusetts Ave & Dupont Circle NW\r\n",
      "   9388 Lincoln Memorial\r\n",
      "   8138 Jefferson Dr & 14th St SW\r\n",
      "   7479 Thomas Circle\r\n",
      "   7401 15th & P St NW\r\n",
      "   6568 14th & V St NW\r\n",
      "   6491 New Hampshire Ave & T St NW\r\n",
      "   5649 Eastern Market Metro / Pennsylvania Ave & 7th St SE\r\n",
      "   5514 17th & Corcoran St NW\r\n"
     ]
    }
   ],
   "source": [
    "!cat output1.csv |\\\n",
    "    uniq -c |\\\n",
    "    sort -rn |\\\n",
    "    head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Save sorted list of [end stations] as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!csvcut -c7 2016q1.csv |\\\n",
    "    sort > output2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Return top 10 most popular destination stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13880 Columbus Circle / Union Station\r\n",
      "  11183 Massachusetts Ave & Dupont Circle NW\r\n",
      "   9419 Lincoln Memorial\r\n",
      "   8975 Jefferson Dr & 14th St SW\r\n",
      "   8092 15th & P St NW\r\n",
      "   7267 14th & V St NW\r\n",
      "   6997 Thomas Circle\r\n",
      "   6245 New Hampshire Ave & T St NW\r\n",
      "   5761 5th & K St NW\r\n",
      "   5651 17th & Corcoran St NW\r\n"
     ]
    }
   ],
   "source": [
    "!cat output2.csv |\\\n",
    "    uniq -c |\\\n",
    "    sort -rn |\\\n",
    "    head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part B: For the most popular departure station, which 10 bikes were used most in trips departing from there? Which\n",
    "10 bikes were used most in trips ending at the most popular destination station?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Save sorted list of bikes associated with most popular departure station 'Columbus Circle / Union Station' as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!csvcut -c5,8 2016q1.csv |\\\n",
    "    csvgrep -c1 -m 'Columbus Circle / Union Station' |\\\n",
    "    csvcut -c2 |\\\n",
    "    sort > output3.csv\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Return 10 most popular bike IDs associated with departure station 'Columbus Circle / Union Station' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17 W22227\r\n",
      "     16 W21867\r\n",
      "     16 W21641\r\n",
      "     16 W21538\r\n",
      "     16 W21239\r\n",
      "     16 W20540\r\n",
      "     16 W00714\r\n",
      "     15 W22080\r\n",
      "     15 W21450\r\n",
      "     15 W21076\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat output3.csv |\\\n",
    "    uniq -c |\\\n",
    "    sort -rn |\\\n",
    "    head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Save sorted list of bikes associated with most popular destination station 'Columbus Circle / Union Station' as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!csvcut -c7,8 2016q1.csv |\\\n",
    "    csvgrep -c1 -m 'Columbus Circle / Union Station' |\\\n",
    "    csvcut -c2 |\\\n",
    "    sort > output4.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Return 10 most popular bike IDs associated with destination station 'Columbus Circle / Union Station' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18 W00485\r\n",
      "     17 W22227\r\n",
      "     16 W22099\r\n",
      "     16 W22080\r\n",
      "     16 W21239\r\n",
      "     16 W21076\r\n",
      "     16 W20425\r\n",
      "     16 W00714\r\n",
      "     15 W21997\r\n",
      "     15 W21867\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat output4.csv |\\\n",
    "    uniq -c |\\\n",
    "    sort -rn |\\\n",
    "    head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Problem 3: Filters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part A: Demonstrate a pipeline that performs a count of the top ten unique words in Little Women. This may be\n",
    "exactly the same pipeline we have used before.*      \n",
    "  \n",
    "*Write a Python filter than replaces grep -oE '\\w{2,}' to split lines of text into one word per line, and\n",
    "write an additional Python filter to replace tr '[:upper:]' '[:lower:]' to transform text into lower case.*      \n",
    "\n",
    "*With your two new filters, repeat the original pipeline, and substitute your new filters as appropriate. You\n",
    "should obtain the same results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Top 10 words in Little Women using same pipeline as used before. The only change was changing the value in brackets from 2 to 1, so that it would include 1 character words such as \"A\" and \"I\" in the counts, which is what was returned by the Python code that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8155 and\r\n",
      "   7689 the\r\n",
      "   5152 to\r\n",
      "   4531 a\r\n",
      "   4003 i\r\n",
      "   3523 of\r\n",
      "   3245 her\r\n",
      "   2774 it\r\n",
      "   2503 in\r\n",
      "   2447 you\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!grep -oE '\\w{{1,}}' women.txt |\\\n",
    "    tr '[:upper:]' '[:lower:]' |\\\n",
    "    sort |\\\n",
    "    uniq -c |\\\n",
    "    sort -rn |\\\n",
    "    head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Produce same analysis as above except using Python filter. I did a head and a tail at the end because the counts originally include blanks as words, which had the largest volume. Doing the head then tail excludes that first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x lower.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: write failed: standard output: Broken pipe\r\n",
      "   8098 and\r\n",
      "   7681 the\r\n",
      "   5148 to\r\n",
      "sort: write error\r\n",
      "   4509 a\r\n",
      "   3518 of\r\n",
      "   3245 her\r\n",
      "   3144 i\r\n",
      "   2500 in\r\n",
      "   2470 it\r\n",
      "   2278 she\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt |\\\n",
    "    ./split.py |\\\n",
    "    ./lower.py |\\\n",
    "    sort |\\\n",
    "    uniq -c |\\\n",
    "    sort -rn |\\\n",
    "    head -11 |\\\n",
    "    tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was unable to get my Python counts to match exactly my grep counts. I believe this is due to differences in how the grep is splitting words opposed to how the Python split function operates. Specifically, I believe that certain special characters that appear next to words are being included in the text of the word. I've attempted to account for this by using a strip command in my split filter which has helped, but gaps still remain. Given more time I would attempt to find a different Python package to be used for splitting text that uses the same logic as the grep command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Part B: Write a Python filter that removes at least ten common words of English text, commonly known as “stop\n",
    "words”. Sources of English stop word lists are readily available online, or you may generate your own list\n",
    "from the text.\n",
    "Add your stop word filter to a word count pipeline and show the top 25 words in Little Women with stop\n",
    "words removed. You may re-use your filters from Part A if you wish, although this is not required for full\n",
    "credit.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the stop word list, I used the list of words originally returned by my Python output since all of the top 10 words returned could technically be defined as stop words. Hence, my population of stop words comprises: and, the, to, a, of, her, i, in, it, she. Below is the new top 25 list when these words are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x stop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2250 you\r\n",
      "   2224 for\r\n",
      "   2034 was\r\n",
      "   1977 as\r\n",
      "   1853 with\r\n",
      "   1848 that\r\n",
      "   1503 he\r\n",
      "   1463 but\r\n",
      "   1247 jo\r\n",
      "   1131 so\r\n",
      "   1116 his\r\n",
      "   1065 at\r\n",
      "   1062 had\r\n",
      "   1012 be\r\n",
      "    974 on\r\n",
      "    942 not\r\n",
      "    916 if\r\n",
      "    873 all\r\n",
      "    843 my\r\n",
      "    826 said\r\n",
      "    819 is\r\n",
      "    781 him\r\n",
      "    749 me\r\n",
      "    731 little\r\n",
      "    715 have\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt |\\\n",
    "    ./split.py |\\\n",
    "    ./lower.py |\\\n",
    "    ./stop.py |\\\n",
    "    sort |\\\n",
    "    uniq -c |\\\n",
    "    sort -rn |\\\n",
    "    head -25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
